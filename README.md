DATA_VISUALIZATION PROJECT
Overview

The Data Visualization Project is designed to extract, process, and visualize data from multiple sources such as SQL Server and REST APIs.
The system follows an automated ELT (Extract – Load – Transform) process, orchestrated entirely by Apache Airflow, with dbt transformations triggered automatically inside the DAG.

All services are containerized and managed with Docker Compose, ensuring easy deployment and consistent environments.

Technologies Used

Apache Airflow – Workflow orchestration and automation for ETL processes.

dbt (Data Build Tool) – Handles data transformations and model creation within the warehouse.

PostgreSQL – Acts as the data warehouse.

pgAdmin – Client tool to monitor and query PostgreSQL.

SQL Server – Source system containing the raw data backup.

Docker Compose – Used to spin up all components in a unified environment.

.env file – Stores all configuration variables and credentials.

Project Structure
DATA_VISUALIZATION/
├── airflow/
│   ├── config/                 # Airflow configuration files
│   ├── dags/                   # DAG definitions for ETL workflows
│   │   └── etl_multi_source.py # Main ETL DAG
│   ├── logs/                   # Airflow logs
│   ├── plugins/                # Custom Airflow plugins (if any)
│   └── Dockerfile              # Dockerfile for the Airflow service
│
├── backup/                     # Database backup files (e.g., loan_raw.bak)
│
├── dbt_project/
│   ├── dbt_packages/           # dbt package dependencies
│   ├── logs/                   # dbt logs
│   ├── macros/                 # Custom dbt macros
│   ├── models/                 # Data models
│   │   ├── marts/              # Fact and dimension models
│   │   └── staging/            # Staging (raw transformation) models
│   ├── profiles/               # dbt connection profiles
│   │   ├── .user.yml
│   │   └── profiles.yml
│   ├── target/                 # Compiled dbt artifacts
│   ├── dbt_project.yml         # dbt project configuration
│   └── package-lock.yml        # Optional dependency lock file
│
├── .env                        # Environment variables
├── .env.example                # Example environment variable template
├── .gitignore                  # Ignored files for Git
├── docker-compose.yml          # Docker Compose configuration
└── README.md                   # This documentation file

Setup and Execution
1. Clone the Repository
git clone https://github.com/<your-username>/loan-intelligence-platform.git
cd data_visualization

2. Configure Environment Variables

Create the .env file from the example:

cp .env.example .env


Then update the values inside .env to match your system (database credentials, ports, etc.).

3. Start the Environment

Run the following command to build and start all containers:

docker-compose up -d --build


This will automatically set up:

Apache Airflow

SQL Server

PostgreSQL

pgAdmin

dbt (integrated with Airflow)

4. Restore the SQL Server Database

Once all containers are running:

Access the SQL Server container.

Restore the backup file located in backup/loan_raw.bak:

RESTORE DATABASE loan_raw 
FROM DISK = '/var/opt/mssql/backup/loan_raw.bak' 
WITH MOVE 'loan_raw' TO '/var/opt/mssql/data/loan_raw.mdf',
     MOVE 'loan_raw_log' TO '/var/opt/mssql/data/loan_raw.ldf',
     REPLACE;


Verify that the database has been restored successfully.

5. Run the Automated ETL Pipeline in Airflow

Open Airflow Web UI at http://localhost:8080
.

Log in using the credentials defined in your .env file.

In the DAGs tab, locate etl_multi_source.

Unpause the DAG and click Run.

The DAG will automatically:

Extract data from SQL Server and REST APIs.

Load raw data into PostgreSQL.

Trigger dbt transformations to build fact and dimension tables automatically (no manual dbt run needed).

You can monitor the execution and logs directly in the Airflow UI.

6. Monitor Data in PostgreSQL via pgAdmin

Open pgAdmin at http://localhost:5050
.

Log in using your PostgreSQL credentials.

Browse schemas, tables, and query data generated by the pipeline.

End-to-End Data Flow
SQL Server (loan_raw) + REST API
        │
        ▼
  Apache Airflow DAG (etl_multi_source)
        │
        ▼
  PostgreSQL Data Warehouse
        │
        ▼
  dbt (automatically triggered)
        │
        ▼
  Fact and Dimension Tables for Analytics

Expected Outcome

After running the pipeline:

The raw data from SQL Server and REST APIs is automatically extracted and loaded into PostgreSQL.

The Airflow DAG triggers dbt, which builds fact and dimension tables automatically.

All transformed data is available in PostgreSQL for reporting, visualization, or further analytics through BI tools or pgAdmin.

No manual dbt commands (dbt run, dbt test, dbt docs) are required — the Airflow DAG handles everything end-to-end.

License

This project is licensed under the MIT License.
You are free to use, modify, and distribute it for educational or professional purposes.