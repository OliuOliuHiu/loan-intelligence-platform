# DATA_VISUALIZATION PROJECT

## ğŸ§© Overview

The **Data Visualization Project** is designed to extract, process, and visualize data from multiple sources such as **SQL Server** and **REST APIs**.

It follows a fully automated **ELT (Extract â€“ Load â€“ Transform)** pipeline, orchestrated by **Apache Airflow**, with **dbt** transformations triggered directly within the DAG.

All components are containerized and managed using **Docker Compose** for easy deployment and consistent environments.

---

## ğŸ”§ Technologies Used

- **Apache Airflow** â€“ Workflow orchestration and ETL automation  
- **dbt (Data Build Tool)** â€“ Data transformation & model building  
- **PostgreSQL** â€“ Data Mart  
- **pgAdmin** â€“ UI tool to monitor/query PostgreSQL  
- **SQL Server** â€“ Raw data source (restored from `.bak`)  
- **Docker Compose** â€“ Container orchestration  
- **`.env`** â€“ Store all configuration variables and credentials  

---

## Project Structure

```
DATA_VISUALIZATION/
â”œâ”€â”€ airflow/                  # Airflow-related configs and DAGs
â”‚   â”œâ”€â”€ config/              # Airflow configuration files
â”‚   â”œâ”€â”€ dags/                # DAG definitions
â”‚   â”‚   â””â”€â”€ etl_multi_source.py
â”‚   â”œâ”€â”€ logs/                # Airflow logs
â”‚   â”œâ”€â”€ plugins/             # Optional custom plugins
â”‚   â””â”€â”€ Dockerfile           # Airflow service image
â”‚
â”œâ”€â”€ backup/                  # SQL Server backup files (.bak)
â”‚
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ dbt_packages/        # Package dependencies
â”‚   â”œâ”€â”€ logs/                # dbt logs
â”‚   â”œâ”€â”€ macros/              # Custom dbt macros
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ marts/           # Fact/dimension models
â”‚   â”‚   â””â”€â”€ staging/         # Staging models
â”‚   â”œâ”€â”€ profiles/
â”‚   â”‚   â”œâ”€â”€ .user.yml
â”‚   â”‚   â””â”€â”€ profiles.yml
â”‚   â”œâ”€â”€ target/              # Compiled dbt artifacts
â”‚   â”œâ”€â”€ dbt_project.yml      # Project config
â”‚   â””â”€â”€ package-lock.yml
â”‚
â”œâ”€â”€ .env                     # Environment config (generated from .env.example)
â”œâ”€â”€ .env.example             # Template for environment variables
â”œâ”€â”€ .gitignore               # Files to ignore in Git
â”œâ”€â”€ docker-compose.yml       # Docker Compose configuration
â””â”€â”€ README.md                # This documentation file
```

---

## âš™ï¸ Setup and Execution

### 1. Clone the Repository

```bash
git clone https://github.com/<your-username>/loan-intelligence-platform.git
cd data_visualization
```

### 2. Configure Environment Variables

```bash
cp .env.example .env
```

> Then update `.env` with your own values (e.g., DB credentials, ports, etc.).

### 3. Start the Docker Environment

```bash
docker-compose up -d --build
```

> This spins up:
> - Apache Airflow
> - SQL Server (with backup restore)
> - PostgreSQL
> - pgAdmin
> - dbt (triggered via Airflow)

---

## Restoring SQL Server Database

After starting all containers:

```sql
-- Inside the SQL Server container:
RESTORE DATABASE loan_raw 
FROM DISK = '/var/opt/mssql/backup/loan_raw.bak' 
WITH MOVE 'loan_raw' TO '/var/opt/mssql/data/loan_raw.mdf',
     MOVE 'loan_raw_log' TO '/var/opt/mssql/data/loan_raw.ldf',
     REPLACE;
```

> Ensure the database `loan_raw` is restored successfully.

---

## ğŸš€ Run ETL Pipeline in Airflow

1. Access Airflow: [http://localhost:8080](http://localhost:8080)  
2. Log in using credentials in `.env`  
3. Locate the DAG: `etl_multi_source`  
4. **Unpause** and **Run** the DAG  

ğŸ” The DAG will:

- Extract data from SQL Server & REST APIs  
- Load raw data into PostgreSQL  
- Trigger dbt to transform staging â†’ fact/dim models  

You can monitor task runs in the Airflow UI.

---

## Monitor Data in pgAdmin

1. Access pgAdmin: [http://localhost:5050](http://localhost:5050)  
2. Log in using PostgreSQL credentials  
3. Browse schemas & tables to validate results

---

## Data Pipeline Architecture

Below is a high-level architecture of the ELT process and orchestration:

![Data Pipeline](pipelineE2E.png)

> Extract from SQL Server and REST API â†’ Load to PostgreSQL â†’ Transform with dbt â†’ Visualize in Power BI  
> All orchestrated by Apache Airflow and containerized with Docker.

---

## Expected Outcome

- Full automation from extraction to transformation  
- No need to run `dbt run`, `dbt test`, etc. manually  
- PostgreSQL becomes the final store for BI / analytics  
- Easy-to-monitor pipeline with centralized orchestration

---

## ğŸªª License

Licensed under the **MIT License**.  
Free to use, modify, and distribute for educational or professional purposes.

---
